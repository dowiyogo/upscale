{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8cc3776",
   "metadata": {},
   "source": [
    "# Real-ESRGAN x4 ‚Äî ONNX + TensorRT 10 (v15)\n",
    "\n",
    "**v15: Usa `trtexec` (CLI de NVIDIA)** para construir el engine, evitando conflictos de Python bindings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "INPUT_DRIVE_DIR   = \"/content/drive/MyDrive/zanelli_miranda\"\n",
    "OUTPUT_DRIVE_ROOT = \"/content/drive/MyDrive/zanelli\"\n",
    "OUTPUT_SUBDIR     = \"upscaled_x4_batches\"\n",
    "FILES_PER_BATCH   = 600\n",
    "ARCHIVE_ZSTD_LVL  = 3\n",
    "MAX_PARALLEL_IO   = 2\n",
    "TRT_PRECISION     = \"fp16\"\n",
    "INPUT_W, INPUT_H  = 720, 480\n",
    "\n",
    "# === NUEVAS L√çNEAS ===\n",
    "ENABLE_FP16       = True\n",
    "JPG_QUALITY       = 95\n",
    "WRITER_THREADS    = 4\n",
    "# =====================\n",
    "\n",
    "CACHE_DRIVE_DIR   = f\"{OUTPUT_DRIVE_ROOT}/.cache_realesrgan_onnx\"\n",
    "ONNX_CACHE_NAME   = f\"realesrgan_{INPUT_W}x{INPUT_H}.onnx\"\n",
    "ENGINE_CACHE_NAME = f\"realesrgan_{INPUT_W}x{INPUT_H}_{TRT_PRECISION}.engine\"\n",
    "WORK_LOCAL_ROOT   = \"/content/work_realesrgan\"\n",
    "LOCAL_IN_DIR      = f\"{WORK_LOCAL_ROOT}/in\"\n",
    "LOCAL_OUT_DIR     = f\"{WORK_LOCAL_ROOT}/out\"\n",
    "LOCAL_TMP_DIR     = f\"{WORK_LOCAL_ROOT}/tmp\"\n",
    "MANIFEST_PATH     = f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}/manifest_{INPUT_W}x{INPUT_H}.txt\"\n",
    "CKPT_PATH         = f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}/checkpoint_{INPUT_W}x{INPUT_H}.json\"\n",
    "\n",
    "import os\n",
    "for k,v in list(locals().items()):\n",
    "    if isinstance(v, (str,int)) and k.isupper(): os.environ[k] = str(v)\n",
    "print(\"‚úÖ Config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff02a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L\n",
    "print(f\"Input dir exists: {os.path.isdir(INPUT_DRIVE_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2985986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, glob\n",
    "os.makedirs(f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}\", exist_ok=True)\n",
    "os.makedirs(CACHE_DRIVE_DIR, exist_ok=True)\n",
    "for d in (WORK_LOCAL_ROOT, LOCAL_IN_DIR, LOCAL_OUT_DIR, LOCAL_TMP_DIR): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "if not os.path.isfile(MANIFEST_PATH):\n",
    "    files = sorted([os.path.basename(p) for p in glob.glob(f\"{INPUT_DRIVE_DIR}/*.png\")])\n",
    "    if not files: raise RuntimeError(f\"No PNG in {INPUT_DRIVE_DIR}\")\n",
    "    open(MANIFEST_PATH, \"w\").write(\"\\n\".join(files)+\"\\n\")\n",
    "    print(f\"Manifest: {len(files)} files\")\n",
    "else:\n",
    "    files = [l.strip() for l in open(MANIFEST_PATH) if l.strip()]\n",
    "    print(f\"Manifest exists: {len(files)} files\")\n",
    "\n",
    "if not os.path.isfile(CKPT_PATH):\n",
    "    json.dump({\"next_index\": 0}, open(CKPT_PATH, \"w\"))\n",
    "print(f\"Checkpoint: {json.load(open(CKPT_PATH)).get('next_index', 0)}\")\n",
    "print(f\"Batches: {len(glob.glob(f'{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}/batch_*.tar.zst'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -qq && apt-get install -y -qq zstd rsync >/dev/null && echo \"‚úÖ APT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278de9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Celda 6) TensorRT en Colab: evitar mismatch Driver/CUDA Runtime\n",
    "# Este notebook puede correr con distintos \"CUDA Version\" seg√∫n el runtime.\n",
    "# Si instalas TensorRT/CUDA demasiado nuevo, aparece:\n",
    "#   \"CUDA driver version is insufficient for CUDA runtime version\"\n",
    "#\n",
    "# Recomendaci√≥n: NO uses /usr/bin/trtexec del sistema si trae un CUDA mayor al que soporta el driver.\n",
    "# En su lugar, instala TensorRT Python en la variante correcta (cu12 o cu13) seg√∫n `nvidia-smi`.\n",
    "\n",
    "import re, subprocess, sys\n",
    "\n",
    "print(\"üîé GPU / Driver / CUDA (seg√∫n nvidia-smi):\")\n",
    "try:\n",
    "    smi = subprocess.check_output([\"nvidia-smi\"]).decode(\"utf-8\", errors=\"ignore\")\n",
    "    print(smi.splitlines()[0])\n",
    "    m = re.search(r\"CUDA Version:\\s+(\\d+)\\.(\\d+)\", smi)\n",
    "    if not m:\n",
    "        raise RuntimeError(\"No pude parsear CUDA Version desde nvidia-smi\")\n",
    "    CUDA_MAJOR = int(m.group(1))\n",
    "    CUDA_MINOR = int(m.group(2))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è No pude ejecutar nvidia-smi:\", e)\n",
    "    print(\"   Si no tienes GPU habilitada: Runtime -> Change runtime type -> GPU\")\n",
    "    CUDA_MAJOR, CUDA_MINOR = 12, 0  # fallback razonable\n",
    "\n",
    "print(f\"‚úÖ Detectado CUDA Version ~ {CUDA_MAJOR}.{CUDA_MINOR} (m√°ximo soportado por el driver).\")\n",
    "\n",
    "# Instalar TensorRT Python en la variante adecuada:\n",
    "# - Si nvidia-smi reporta CUDA 12.x -> tensorrt-cu12\n",
    "# - Si reporta CUDA 13.x -> tensorrt-cu13\n",
    "# Nota: por defecto, algunos paquetes instalan CUDA 13.x; aqu√≠ forzamos la variante que calza con el driver.\n",
    "pkg = f\"tensorrt-cu{CUDA_MAJOR}\"\n",
    "print(\"üì¶ Instalando:\", pkg)\n",
    "!pip -q install --upgrade \"{pkg}\" onnx\n",
    "\n",
    "import tensorrt as trt\n",
    "print(\"‚úÖ TensorRT Python:\", trt.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e57827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RRDBNet architecture (compatibles con pesos RealESRGAN_x4plus.pth)\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class ResidualDenseBlock5C(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nf + 2*gc, gc, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nf + 3*gc, gc, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nf + 4*gc, nf, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super().__init__()\n",
    "        # Nombres IMPORTANTES para que calcen con el state_dict:\n",
    "        # rdb1/2/3 y conv1..conv5\n",
    "        self.rdb1 = ResidualDenseBlock5C(nf, gc)\n",
    "        self.rdb2 = ResidualDenseBlock5C(nf, gc)\n",
    "        self.rdb3 = ResidualDenseBlock5C(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rdb1(x)\n",
    "        out = self.rdb2(out)\n",
    "        out = self.rdb3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, num_in_ch=3, num_out_ch=3, nf=64, nb=23, gc=32):\n",
    "        super().__init__()\n",
    "        self.conv_first = nn.Conv2d(num_in_ch, nf, 3, 1, 1)\n",
    "        self.body = nn.Sequential(*[RRDB(nf, gc) for _ in range(nb)])\n",
    "        self.conv_body = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "\n",
    "        # upsample x4 (nearest + conv) como en ESRGAN / Real-ESRGAN\n",
    "        self.conv_up1 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.conv_up2 = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.conv_hr  = nn.Conv2d(nf, nf, 3, 1, 1)\n",
    "        self.conv_last = nn.Conv2d(nf, num_out_ch, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        body_feat = self.conv_body(self.body(feat))\n",
    "        feat = feat + body_feat\n",
    "        feat = self.lrelu(self.conv_up1(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        feat = self.lrelu(self.conv_up2(F.interpolate(feat, scale_factor=2, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.conv_hr(feat)))\n",
    "        return out\n",
    "\n",
    "print(\"‚úÖ RRDBNet (nombres compatibles con RealESRGAN_x4plus)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cbffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export ONNX (CPU only) ‚Äî forzar pesos embebidos (sin external_data)\n",
    "import os, torch\n",
    "# Asegurar dependencias para exporter ONNX (PyTorch reciente usa onnxscript)\n",
    "import importlib, sys, subprocess, inspect\n",
    "\n",
    "def _pip_install(*pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "for pkg in (\"onnx\", \"onnxscript\"):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ImportError:\n",
    "        _pip_install(pkg)\n",
    "\n",
    "ONNX_LOCAL = f\"{LOCAL_TMP_DIR}/realesrgan.onnx\"\n",
    "ONNX_CACHE = f\"{CACHE_DRIVE_DIR}/{ONNX_CACHE_NAME}\"\n",
    "\n",
    "if os.path.isfile(ONNX_CACHE):\n",
    "    print(\"üîÅ ONNX cached\")\n",
    "    !cp \"{ONNX_CACHE}\" \"{ONNX_LOCAL}\"\n",
    "else:\n",
    "    print(\"Exporting ONNX...\")\n",
    "    m = RRDBNet()\n",
    "    pth = f\"{LOCAL_TMP_DIR}/weights.pth\"\n",
    "    if not os.path.isfile(pth):\n",
    "        !wget -q -O \"{pth}\" \"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\"\n",
    "    sd = torch.load(pth, map_location='cpu', weights_only=True)\n",
    "    state = sd.get('params_ema', sd.get('params', sd))\n",
    "\n",
    "    # Algunos checkpoints vienen con prefijo 'module.' (DataParallel)\n",
    "    if isinstance(state, dict) and any(k.startswith('module.') for k in state.keys()):\n",
    "        state = {k.replace('module.', '', 1): v for k, v in state.items()}\n",
    "\n",
    "    try:\n",
    "        m.load_state_dict(state, strict=True)\n",
    "    except RuntimeError:\n",
    "        print(\"‚ùå load_state_dict strict=True fall√≥. Diagn√≥stico con strict=False:\")\n",
    "        missing, unexpected = m.load_state_dict(state, strict=False)\n",
    "        print(\"  missing keys:\", len(missing))\n",
    "        print(\"  unexpected keys:\", len(unexpected))\n",
    "        print(\"  ejemplo missing:\", missing[:5])\n",
    "        print(\"  ejemplo unexpected:\", unexpected[:5])\n",
    "        raise\n",
    "\n",
    "    # Asegurar float32 en export (evita pesos BF16/FP16 raros)\n",
    "    m = m.float().cpu().eval()\n",
    "\n",
    "    dummy = torch.randn(1, 3, INPUT_H, INPUT_W, dtype=torch.float32)\n",
    "\n",
    "    # Export: usar exporter cl√°sico si est√° disponible (evita depender de dynamo)\n",
    "    _export_kwargs = dict(\n",
    "        opset_version=17,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        keep_initializers_as_inputs=False,\n",
    "    )\n",
    "    _sig = inspect.signature(torch.onnx.export)\n",
    "    if 'dynamo' in _sig.parameters:\n",
    "        _export_kwargs['dynamo'] = False  # exporter cl√°sico\n",
    "\n",
    "    torch.onnx.export(\n",
    "        m,\n",
    "        dummy,\n",
    "        ONNX_LOCAL,\n",
    "        **_export_kwargs,\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ ONNX {os.path.getsize(ONNX_LOCAL)/1e6:.1f}MB\")\n",
    "\n",
    "    # Re-guardar con onnx para garantizar que NO use external data (opcional)\n",
    "    try:\n",
    "        import onnx\n",
    "        mm = onnx.load_model(ONNX_LOCAL, load_external_data=True)\n",
    "        onnx.save_model(mm, ONNX_LOCAL, save_as_external_data=False)\n",
    "        print(\"‚úÖ ONNX re-guardado sin external data\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è  No se pudo re-guardar ONNX con el paquete onnx (opcional).\", type(e).__name__)\n",
    "\n",
    "    !cp \"{ONNX_LOCAL}\" \"{ONNX_CACHE}\"\n",
    "    del m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Celda 9) Build TensorRT engine (TensorRT Python) ‚Äî robusto para ONNX con pesos externos\n",
    "import os, time, glob\n",
    "\n",
    "ENABLE_FP16 = True  # Usar FP16 para m√°xima velocidad\n",
    "\n",
    "ENGINE_LOCAL = f\"{LOCAL_TMP_DIR}/realesrgan.engine\"\n",
    "ENGINE_CACHE = f\"{CACHE_DRIVE_DIR}/{ENGINE_CACHE_NAME}\"\n",
    "\n",
    "def _file_mb(p):\n",
    "    try: return os.path.getsize(p)/1024/1024\n",
    "    except: return -1\n",
    "\n",
    "if os.path.isfile(ENGINE_CACHE):\n",
    "    print(\"üîÅ Engine cached\")\n",
    "    !cp \"{ENGINE_CACHE}\" \"{ENGINE_LOCAL}\"\n",
    "else:\n",
    "    print(\"Building TensorRT engine with TensorRT Python (puede tardar 2-15 min)...\")\n",
    "\n",
    "    # 1) Diagn√≥stico r√°pido del ONNX (especialmente si fue exportado con external data)\n",
    "    onnx_dir = os.path.dirname(ONNX_LOCAL)\n",
    "    onnx_base = os.path.splitext(os.path.basename(ONNX_LOCAL))[0]\n",
    "    sidecars = sorted(glob.glob(os.path.join(onnx_dir, onnx_base + \"*\")))\n",
    "    print(f\"ONNX: {ONNX_LOCAL}  ({_file_mb(ONNX_LOCAL):.1f} MB)\")\n",
    "    print(\"Sidecar files:\", [os.path.basename(x) for x in sidecars])\n",
    "\n",
    "    # 2) Si el ONNX usa pesos externos (archivo .data/.onnx_data/etc), crear copia EMBEBIDA\n",
    "    #    Esto evita el fallo t√≠pico: \"Failed to import initializer: <weight>\"\n",
    "    ONNX_TO_PARSE = ONNX_LOCAL\n",
    "    ONNX_EMBED = os.path.join(onnx_dir, onnx_base + \"_embedded.onnx\")\n",
    "\n",
    "    try:\n",
    "        import onnx\n",
    "        try:\n",
    "            m = onnx.load_model(ONNX_LOCAL, load_external_data=True)\n",
    "            # Guardar embebido (sin external data)\n",
    "            onnx.save_model(m, ONNX_EMBED, save_as_external_data=False)\n",
    "            if os.path.isfile(ONNX_EMBED) and os.path.getsize(ONNX_EMBED) > 0:\n",
    "                ONNX_TO_PARSE = ONNX_EMBED\n",
    "                print(f\"‚úÖ ONNX embebido creado: {ONNX_TO_PARSE}  ({_file_mb(ONNX_TO_PARSE):.1f} MB)\")\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è  No pude cargar/embeber external data con onnx.load_model(load_external_data=True).\")\n",
    "            print(\"    Si tu export gener√≥ un archivo de pesos externo (p.ej. *.data / *.onnx_data), aseg√∫rate que exista en el mismo directorio.\")\n",
    "            print(\"    Error:\", type(e).__name__, str(e)[:300])\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è  Paquete 'onnx' no disponible; se intentar√° parsear el ONNX tal cual.\")\n",
    "        print(\"    Error:\", type(e).__name__, str(e)[:200])\n",
    "\n",
    "    import tensorrt as trt\n",
    "\n",
    "    t0 = time.time()\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "    # Crear builder / network / parser\n",
    "    builder = trt.Builder(logger)\n",
    "    explicit = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    network = builder.create_network(explicit)\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "    # 3) Parse: preferir parse_from_file (maneja mejor external-data que parse(bytes))\n",
    "    ok = False\n",
    "    if hasattr(parser, \"parse_from_file\"):\n",
    "        ok = parser.parse_from_file(ONNX_TO_PARSE)\n",
    "    else:\n",
    "        with open(ONNX_TO_PARSE, \"rb\") as f:\n",
    "            ok = parser.parse(f.read())\n",
    "\n",
    "    if not ok:\n",
    "        print(\"‚ùå Error parseando ONNX en TensorRT:\")\n",
    "        for i in range(parser.num_errors):\n",
    "            print(parser.get_error(i))\n",
    "\n",
    "        # Mensaje espec√≠fico para el error t√≠pico de initializers\n",
    "        print(\"\\nSugerencias r√°pidas:\")\n",
    "        print(\" - Si el error menciona 'Failed to import initializer: ...', casi siempre es porque el ONNX fue guardado con pesos externos y falta el archivo sidecar.\")\n",
    "        print(\" - Revisa que junto a realesrgan.onnx exista el archivo de pesos (p.ej. realesrgan.onnx.data / realesrgan.data / *.onnx_data).\")\n",
    "        print(\" - Si existe, aseg√∫rate que est√© en el MISMO directorio que el .onnx.\")\n",
    "        print(\" - Alternativa: re-exporta el ONNX con pesos embebidos (sin external_data).\")\n",
    "        raise RuntimeError(\"ONNX parse failed\")\n",
    "\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # Workspace\n",
    "    workspace_gib = 4\n",
    "    try:\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace_gib * (1 << 30))\n",
    "    except Exception:\n",
    "        config.max_workspace_size = workspace_gib * (1 << 30)\n",
    "\n",
    "    # FP16\n",
    "    if ENABLE_FP16:\n",
    "        try:\n",
    "            if builder.platform_has_fast_fp16:\n",
    "                config.set_flag(trt.BuilderFlag.FP16)\n",
    "                print(\"  FP16 enabled\")\n",
    "            else:\n",
    "                print(\"  FP16 requested but platform_has_fast_fp16=False; building FP32\")\n",
    "        except Exception:\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "            print(\"  FP16 enabled (sin check)\")\n",
    "\n",
    "    # Build\n",
    "    engine_bytes = builder.build_serialized_network(network, config)\n",
    "    if engine_bytes is None:\n",
    "        raise RuntimeError(\"TensorRT build failed (engine_bytes is None)\")\n",
    "\n",
    "    with open(ENGINE_LOCAL, \"wb\") as f:\n",
    "        f.write(engine_bytes)\n",
    "\n",
    "    print(f\"‚úÖ Engine built: {ENGINE_LOCAL}  ({time.time()-t0:.1f} s)\")\n",
    "    !cp \"{ENGINE_LOCAL}\" \"{ENGINE_CACHE}\"\n",
    "\n",
    "# sanity check\n",
    "if not os.path.isfile(ENGINE_LOCAL) or os.path.getsize(ENGINE_LOCAL) < 1024*1024:\n",
    "    raise RuntimeError(\"Engine file missing or too small ‚Äî build likely failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b66b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pycuda for inference\n",
    "!pip install -q pycuda\n",
    "print(\"‚úÖ pycuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f8265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference class using TensorRT Runtime (not Builder)\n",
    "import numpy as np, cv2\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "# Import tensorrt AFTER pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        # Cargar engine (solo Runtime, no Builder)\n",
    "        with open(path, 'rb') as f:\n",
    "            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n",
    "        \n",
    "        if self.engine is None:\n",
    "            raise RuntimeError(\"Failed to load engine\")\n",
    "        \n",
    "        self.ctx = self.engine.create_execution_context()\n",
    "        self.stream = cuda.Stream()\n",
    "        \n",
    "        # Get tensor info\n",
    "        self.iname = self.engine.get_tensor_name(0)\n",
    "        self.oname = self.engine.get_tensor_name(1)\n",
    "        self.ishape = tuple(self.engine.get_tensor_shape(self.iname))\n",
    "        self.oshape = tuple(self.engine.get_tensor_shape(self.oname))\n",
    "        \n",
    "        # Allocate GPU memory\n",
    "        self.d_in = cuda.mem_alloc(int(np.prod(self.ishape)) * 4)\n",
    "        self.d_out = cuda.mem_alloc(int(np.prod(self.oshape)) * 4)\n",
    "        self.h_out = np.empty(self.oshape, np.float32)\n",
    "        \n",
    "        print(f\"In: {self.ishape}, Out: {self.oshape}\")\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Preprocess: BGR->RGB, HWC->NCHW, normalize\n",
    "        x = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        x = np.ascontiguousarray(x.transpose(2, 0, 1)[None])\n",
    "        \n",
    "        # Copy to GPU\n",
    "        cuda.memcpy_htod_async(self.d_in, x, self.stream)\n",
    "        \n",
    "        # Execute\n",
    "        self.ctx.set_tensor_address(self.iname, int(self.d_in))\n",
    "        self.ctx.set_tensor_address(self.oname, int(self.d_out))\n",
    "        self.ctx.execute_async_v3(self.stream.handle)\n",
    "        \n",
    "        # Copy back\n",
    "        cuda.memcpy_dtoh_async(self.h_out, self.d_out, self.stream)\n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        # Postprocess: NCHW->HWC, denormalize, RGB->BGR\n",
    "        y = np.clip(self.h_out[0], 0, 1).transpose(1, 2, 0)\n",
    "        return cv2.cvtColor((y * 255).round().astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "model = Model(ENGINE_LOCAL)\n",
    "print(\"‚úÖ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test\n",
    "import shutil, time, glob\n",
    "shutil.rmtree(LOCAL_IN_DIR, True); shutil.rmtree(LOCAL_OUT_DIR, True)\n",
    "os.makedirs(LOCAL_IN_DIR); os.makedirs(LOCAL_OUT_DIR)\n",
    "\n",
    "for f in files[:5]: \n",
    "    shutil.copy2(f\"{INPUT_DRIVE_DIR}/{f}\", LOCAL_IN_DIR)\n",
    "\n",
    "print(f\"Processing {len(files[:5])} images...\")\n",
    "t0 = time.time()\n",
    "for f in files[:5]:\n",
    "    img = cv2.imread(f\"{LOCAL_IN_DIR}/{f}\")\n",
    "    if img is not None: \n",
    "        cv2.imwrite(f\"{LOCAL_OUT_DIR}/{f}\", model(img))\n",
    "\n",
    "dt = time.time() - t0\n",
    "outs = glob.glob(LOCAL_OUT_DIR + '/*.png')\n",
    "print(f\"‚úÖ {len(outs)} imgs, {dt:.1f}s total, {dt/max(1,len(outs)):.2f}s/img\")\n",
    "\n",
    "if outs:\n",
    "    sin = cv2.imread(f\"{LOCAL_IN_DIR}/{files[0]}\")\n",
    "    sout = cv2.imread(outs[0])\n",
    "    print(f\"   Input:  {sin.shape}\")\n",
    "    print(f\"   Output: {sout.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e701cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop - CORREGIDO\n",
    "import json, subprocess, shutil, time, glob, threading, queue, os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "OUT = f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}\"\n",
    "\n",
    "def bash(c): return subprocess.check_output(c, shell=True, text=True)\n",
    "def ckpt(): return json.load(open(CKPT_PATH))\n",
    "def save_ckpt(i, extra=None):\n",
    "    d = ckpt(); d['next_index'] = i\n",
    "    if extra: d.update(extra)\n",
    "    json.dump(d, open(CKPT_PATH, 'w'), indent=2)\n",
    "\n",
    "def bname(s, e): return f\"batch_{s:08d}_{e:08d}.tar.zst\"\n",
    "def done(s, e): return os.path.isfile(f\"{OUT}/{bname(s,e)}\")\n",
    "\n",
    "def format_time(seconds):\n",
    "    if seconds < 60: return f\"{seconds:.0f}s\"\n",
    "    elif seconds < 3600: return f\"{seconds/60:.1f} min\"\n",
    "    else: return f\"{int(seconds//3600)}h {int((seconds%3600)//60)}m\"\n",
    "\n",
    "# === FIX #2: Recalcular idx desde el primer batch faltante ===\n",
    "real_idx = 0\n",
    "while real_idx < len(files):\n",
    "    s, e = real_idx, min(len(files), real_idx + FILES_PER_BATCH)\n",
    "    if not done(s, e):\n",
    "        break\n",
    "    real_idx = e\n",
    "idx = real_idx\n",
    "total = len(files)\n",
    "save_ckpt(idx)\n",
    "print(f\"Start: {idx}/{total} (verificado)\")\n",
    "\n",
    "prefetch_pool = ThreadPoolExecutor(1)\n",
    "upload_pool = ThreadPoolExecutor(2)\n",
    "\n",
    "# === WRITER THREADS ===\n",
    "write_queue = queue.Queue(maxsize=WRITER_THREADS * 2)\n",
    "\n",
    "def writer_thread():\n",
    "    while True:\n",
    "        item = write_queue.get()\n",
    "        if item is None:\n",
    "            write_queue.task_done()\n",
    "            break\n",
    "        path, img = item\n",
    "        cv2.imwrite(path, img, [cv2.IMWRITE_JPEG_QUALITY, JPG_QUALITY])\n",
    "        write_queue.task_done()\n",
    "\n",
    "writers = []\n",
    "for _ in range(WRITER_THREADS):\n",
    "    t = threading.Thread(target=writer_thread, daemon=True)\n",
    "    t.start()\n",
    "    writers.append(t)\n",
    "\n",
    "# === UPLOAD ASYNC (FIX #2: checkpoint secuencial) ===\n",
    "upload_queue = queue.Queue()\n",
    "last_uploaded_idx = idx  # Checkpoint secuencial\n",
    "\n",
    "def upload_worker():\n",
    "    global last_uploaded_idx\n",
    "    while True:\n",
    "        item = upload_queue.get()\n",
    "        if item is None:\n",
    "            upload_queue.task_done()\n",
    "            break\n",
    "        arc, name, end_idx = item\n",
    "        try:\n",
    "            bash(f\"rsync -a '{arc}' '{OUT}/{name}'\")\n",
    "            # Solo avanzar checkpoint si es el siguiente en secuencia\n",
    "            if end_idx == last_uploaded_idx + FILES_PER_BATCH or last_uploaded_idx == idx:\n",
    "                last_uploaded_idx = end_idx\n",
    "                save_ckpt(end_idx)\n",
    "            try: os.remove(arc)\n",
    "            except: pass\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Upload error: {e}\")\n",
    "        upload_queue.task_done()\n",
    "\n",
    "upload_thread = threading.Thread(target=upload_worker, daemon=True)\n",
    "upload_thread.start()\n",
    "\n",
    "# === PREFETCH (FIX #1: alternancia correcta) ===\n",
    "LOCAL_IN_A = f\"{WORK_LOCAL_ROOT}/in_A\"\n",
    "LOCAL_IN_B = f\"{WORK_LOCAL_ROOT}/in_B\"\n",
    "os.makedirs(LOCAL_IN_A, exist_ok=True)\n",
    "os.makedirs(LOCAL_IN_B, exist_ok=True)\n",
    "\n",
    "def copy_batch_to_dir(file_list, target_dir):\n",
    "    shutil.rmtree(target_dir, True)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    lst = f\"{LOCAL_TMP_DIR}/pf_{os.path.basename(target_dir)}.txt\"\n",
    "    open(lst, 'w').write(\"\\n\".join(file_list) + \"\\n\")\n",
    "    bash(f\"rsync -a --files-from='{lst}' '{INPUT_DRIVE_DIR}/' '{target_dir}/'\")\n",
    "    return target_dir\n",
    "\n",
    "# Calcular batches pendientes\n",
    "batches = []\n",
    "temp_idx = idx\n",
    "while temp_idx < total:\n",
    "    s, e = temp_idx, min(total, temp_idx + FILES_PER_BATCH)\n",
    "    if not done(s, e):\n",
    "        batches.append((s, e))\n",
    "    temp_idx = e\n",
    "\n",
    "print(f\"Batches pendientes: {len(batches)}\")\n",
    "\n",
    "if not batches:\n",
    "    print(\"‚úÖ Todo ya procesado!\")\n",
    "else:\n",
    "    global_start = time.time()\n",
    "    \n",
    "    # Prefetch inicial a LOCAL_IN_A\n",
    "    s0, e0 = batches[0]\n",
    "    print(f\"üì• Prefetch inicial [{s0}:{e0})...\")\n",
    "    prefetch_future = prefetch_pool.submit(copy_batch_to_dir, files[s0:e0], LOCAL_IN_A)\n",
    "    \n",
    "    # FIX #1: Variables claras para alternancia\n",
    "    use_A_for_current = True  # Batch actual usa A, prefetch va a B\n",
    "\n",
    "    for batch_idx, (s, e) in enumerate(batches):\n",
    "        batch_num = batch_idx + 1\n",
    "        total_batches = len(batches)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üé¨ Batch {batch_num}/{total_batches} [{s}:{e}) ({e-s} frames)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Determinar directorios\n",
    "        current_in_dir = LOCAL_IN_A if use_A_for_current else LOCAL_IN_B\n",
    "        prefetch_target = LOCAL_IN_B if use_A_for_current else LOCAL_IN_A\n",
    "        \n",
    "        # Esperar prefetch de ESTE batch\n",
    "        t0 = time.time()\n",
    "        prefetch_future.result()\n",
    "        wait_time = time.time() - t0\n",
    "        if wait_time > 1:\n",
    "            print(f\"   üì• Esper√≥ prefetch: {wait_time:.1f}s\")\n",
    "        else:\n",
    "            print(f\"   üì• Prefetch listo ‚úì\")\n",
    "        \n",
    "        # Lanzar prefetch del SIGUIENTE batch (a la otra carpeta)\n",
    "        if batch_idx + 1 < len(batches):\n",
    "            next_s, next_e = batches[batch_idx + 1]\n",
    "            prefetch_future = prefetch_pool.submit(copy_batch_to_dir, files[next_s:next_e], prefetch_target)\n",
    "        \n",
    "        # Alternar para pr√≥xima iteraci√≥n\n",
    "        use_A_for_current = not use_A_for_current\n",
    "        \n",
    "        # Preparar output\n",
    "        shutil.rmtree(LOCAL_OUT_DIR, True)\n",
    "        os.makedirs(LOCAL_OUT_DIR)\n",
    "        \n",
    "        # Procesar\n",
    "        t1 = time.time()\n",
    "        n = 0\n",
    "        for f in files[s:e]:\n",
    "            img = cv2.imread(f\"{current_in_dir}/{f}\")\n",
    "            if img is not None:\n",
    "                out_img = model(img)\n",
    "                base, _ = os.path.splitext(f)\n",
    "                write_queue.put((f\"{LOCAL_OUT_DIR}/{base}.jpg\", out_img))\n",
    "                n += 1\n",
    "        \n",
    "        write_queue.join()\n",
    "        dt = time.time() - t1\n",
    "        print(f\"   ‚ö° TRT+JPG: {dt:.1f}s ({n} imgs, {n/dt:.2f} fps)\")\n",
    "        \n",
    "        # Comprimir\n",
    "        t2 = time.time()\n",
    "        arc = f\"{WORK_LOCAL_ROOT}/{bname(s,e)}\"\n",
    "        bash(f\"tar -I 'zstd -{ARCHIVE_ZSTD_LVL} -T0' -cf '{arc}' -C '{LOCAL_OUT_DIR}' .\")\n",
    "        print(f\"   üì¶ Zip: {time.time()-t2:.1f}s\")\n",
    "        \n",
    "        # Encolar upload\n",
    "        upload_queue.put((arc, bname(s,e), e))\n",
    "        print(f\"   üì§ Upload encolado\")\n",
    "        \n",
    "        # Monitor\n",
    "        frames_done = e\n",
    "        elapsed = time.time() - global_start\n",
    "        fps = (frames_done - idx) / elapsed\n",
    "        remaining = total - frames_done\n",
    "        eta = remaining / fps if fps > 0 else 0\n",
    "        \n",
    "        print(f\"\\n   üìä Progreso: {frames_done:,}/{total:,} ({100*frames_done/total:.1f}%)\")\n",
    "        print(f\"   ‚è±Ô∏è  Tiempo: {format_time(elapsed)} | FPS: {fps:.2f}\")\n",
    "        print(f\"   üèÅ ETA: {format_time(eta)}\")\n",
    "\n",
    "    print(\"\\n‚è≥ Esperando uploads...\")\n",
    "    upload_queue.join()\n",
    "\n",
    "# Cleanup\n",
    "upload_queue.put(None)\n",
    "for _ in writers:\n",
    "    write_queue.put(None)\n",
    "write_queue.join()\n",
    "\n",
    "total_time = time.time() - global_start\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ COMPLETADO\")\n",
    "print(f\"   Frames: {total:,}\")\n",
    "print(f\"   Tiempo: {format_time(total_time)}\")\n",
    "print(f\"   FPS: {(total-idx)/total_time:.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract batch (optional)\n",
    "bs = sorted(glob.glob(f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}/batch_*.tar.zst\"))\n",
    "print(f\"{len(bs)} batches\")\n",
    "if bs:\n",
    "    out = f\"{OUTPUT_DRIVE_ROOT}/{OUTPUT_SUBDIR}/extracted\"\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    !tar -I 'zstd -d' -xf \"{bs[0]}\" -C \"{out}\"\n",
    "    print(f\"‚úÖ {len(glob.glob(out+'/*.jpg'))} files\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Upscale_TRT_v15.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
